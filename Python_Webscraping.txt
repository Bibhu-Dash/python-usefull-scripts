
----------------------
The webbrowser Module:
----------------------
>>> import webbrowser
>>> webbrowser.open('https://automatetheboringstuff.com')
True
>>> 

Q> write a script named 'mapit.py' and when you run this script with commandline argument like some address.
   It should open that address in google MAP by opening up browser..

----------------
import webbrowser, sys, pyperclip

sys.argv # ['mapit.py','870','Valencia','St.']

#Check if command line arguments were passed

if len(sys.argv) > 1:
    #['mapit.py','870','Valencia','St.'] -> '870 Valencia St.'
    address = ' '.join(sys.argv[1:])
else:
    address = pyperclip.paste()

#https://www.google.com/maps/place/<address>
webbrowser.open('https://www.google.com/maps/place/' + address)

-----------------------------------------------------
Downloading from the Web with the Requests Module:
-----------------------------------------------------
We can use Request module to download files from Web...

'pip install requests'   -----------> to download the requests module.

-----------
>>> import requests
>>> res = requests.get('https://automatetheboringstuff.com/files/rj.txt')  ---> get() returns a response object.. and we store it in variable
>>> res.status_code                     --------------------------------------> it give the status that file has downloaded properly or not..
200                                     -----------------------> means properly downloaded.
>>> 
>>> mes = requests.get('https://automatetheboringstuff.com/files/rm.txt')
>>> mes.status_code
404                                   -----------------------------------------> Means not found..
>>> 
>>> len(res.text)     -------------------------------------> length of the response object 
178978
>>> print(res.text[:500]) ----------------------------------> we can slice the some portion of text file and print print like this..

rasie_for_status() method for response object:
---------------------------------------------
this method raise an exception if there is any ERROR downloading the file.
And do nothing if download is suceed..

example:
>>> res.raise_for_status()		       
>>> 
>>> mes.raise_for_status()
			       
Traceback (most recent call last):
  File "<pyshell#70>", line 1, in <module>
    mes.raise_for_status()
  File "C:\Users\c_bibhup\AppData\Local\Programs\Python\Python37\lib\site-packages\requests\models.py", line 940, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://automatetheboringstuff.com/files/rm.txt
>>> 

complete download process:
-------------------------
>>> import requests
>>> res = requests.get('https://automatetheboringstuff.com/files/rj.txt')  ---> get() returns a response object.. and we store it in variable
>>> res.status_code                     --------------------------------------> it give the status that file has downloaded properly or not..
200
>>> playfile = open('RomeoNJuliet.txt','wb')			       
>>> for chunk in res.iter_content(100000):
			       playfile.write(chunk)

			       
100000
78978
>>> 
----------------
if you want learn more about response object methods... then : https://requests.readthedocs.org/en/latest/
Note:
the above procedure to download file ... is only possible if you know the path details to downlaod..
But if you want to login to some page and download something.. then you can use selenium module for this... 
will read abot it in next chapter...

Notes:
----------
->The requests module is a third-party module for downloading web pages and files..
->requests.get() returns a Response object.
->The raise_for_status() Resopnse method will raise an exception if the download failed.
->You can save a downloaded file to your hard drive with calls to the iter_content() method.


---------------------------------------------
Parsing HTML with the beautiful Soup module:
---------------------------------------------
every web page is designed with HTML(Hyper Text Markup Language).
we can see the HTML elements of any webpage by pressing (Ctrl+U) or by (Right_click->View_Page_Source)

-----------------------------
import bs4, requests

def getAmazonPrice(productUrl):
	res = request.get(productUrl)
	res.raise_for_status()

	soup = bs4.BeautifulSoup(res.text,'html.parser')
	elems = soup.select('#soldByThirdParty > span.a-size-medium.a-color-price.inlineBlock-display.offer-price.a-text-normal.price3P')
	return elems[0].text.strip()

price = getAmazonPrice('https://www.amazon.in/Automate-Boring-Python-Albert-Sweigart/dp/1593275994/')
print('The price is' +price)
---------------------



Notes:
--------
-> Web Pages are plaintext files formatted as HTML.
-> HTML can be parsed with the  BeautifulSoup module.
-> BeautifulSoup is imported with the same name bs4 .
-> Pass the string with the HTML to the bs4.BeautifulSoup() function to get a Soup object. 
-> The Soup object has a select() method that can be passed a string of the CSS selector for an HTML tag.
-> You can get a CSS selector string from the browser's developer tools by right-clicking the element and selecting Copy CSS Path.
-> The select() method will return a list of matching Element objects.



-------------------------------------------------
Controlling the Browser with the Selenium Module:
-------------------------------------------------
If we required to login/fill forms/click buttons.... then above methods will not be sufficeient 
hence we can use Selenium Module to do all these stuffs...

now we are going to work on firefox web browser

from selenium import webdriver
browser = webdriver.Firefox()   #returns a browser object and opens the browser
browser.get('https://automatetheboaringstuff.com')

###now we can select any link on the web page by using unique CSS selector

elem = browser.find_element_by_css_selector('body > div.main > div:nth-child(1) > ul:nth-child(18) > li:nth-child(1) > a')
elem.click()   ## it can be a check box/button/radio button

--------

how type in search field in webpage

searchElem = browser.find_element_by_css_selector('.search-field')

searchElem.send_keys('zophie')
searchElem.submit()

browser.back()
browser.forward()
browser.refresh()
browser.reload()
browser.quit()

--------
how selenium can help to read some content on the page:

elem = browser.find_element_by_css_selector('body > div.main > div:nth-child(1) > p:nth-child(6)')

if you want to copy whole page:

elem = browser.find_element_by_css_selector('html')

for more selenium knowlwdge with python:  https://selenium-python.readthedocs.org/



Notes:
----------
-> To import selenium, you need to run: from selenium import webdriver
-> To open the browser, run: browser webdriver.Firefox()
-> To send the browser to a URL, run: browser.get(http://inventwithpython.com)
-> The browser.find_elements_by_css_selector() method will return a list of WebElement objects
-> WebElement objects have a text variable that contains the element's HTML in a string.
-> The click() method will click on an element in the browser.
-> The send keys() method will type into a specific element in the browser.
-> The submit() method will simulate clicking on the Submit button for a form.
-> The browser can also be controlled with these methods: back(), forward(), refresh(), quit()
